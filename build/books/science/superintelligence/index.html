<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>Superintelligence - by Nick Bostrom</title>
    <meta name="description" content="Projects and book notes" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:400,700">
    <link rel="stylesheet" type="text/css" href="../../../css/normalize.css">
    <link rel="stylesheet" type="text/css" href="../../../css/styles.css">
    <link rel="icon" type="image/x-icon" href="../../../favicon.ico"></head>

<body>
    <div class="container">
        <header class="navigation-wrapper" role="banner">
            <a href="../../../" class="home-link">
                <h1>FLUID SELF</h1>
                <h1 class="h1-shortened">F/S</h1>
            </a>
            <nav role="navigation">
                <a href="../../../#projects">Projects</a>
                <a href="../../../#about">About</a>
                <a href="../../../books">Books</a>
            </nav>
        </header>        <main class="main-wrapper" role="main">
            <section>
                <div class="section-header">
                    <h2>Superintelligence - by Nick Bostrom</h2>
                </div>
                <p><strong>Past developments and present capabilities</strong></p>
<ul>
<li>10% chance: 2030</li>
<li>50% chance: 2050</li>
<li>90% chance: 2100</li>
</ul>
<p>Small sample sizes, selection biases, and--above all--the inherent unreliability of the subjective opinions elicited mean that one should not read too much into these expert surveys and interviews. They do not let us draw any strong conclusion. But they do hint at a weak conclusion. They suggest that (at least in lieu of better data or analysis) it may be reasonable to believe that human-level machine intelligence has a fairly sizeable chance of being developed by mid-century, and that it has a non-trivial chance of being developed considerably sooner or much later; that it might perhaps fairly soon thereafter result in superintelligence; and that a wide range of outcomes may have a significant chance of occurring, including extremely good outcomes and outcomes that are as bad as human extinction.</p>
<p><strong>Paths to superintelligence</strong></p>
<ul>
<li><p><strong>Artificial intelligence:</strong> &quot;Recursive self-improvement.&quot; A successful seed AI would be able to iteratively enhance itself: an early version of the AI could design an improved version of itself, and the improved version--being smarter than the original--might be able to design an even smarter version of itself, and so forth.20 Under some conditions, such a process of recursive self-improvement might continue long enough to result in an intelligence explosion--an event in which, in a short period of time, a system&#39;s level of intelligence increases from a relatively modest endowment of cognitive capabilities (perhaps sub-human in most respects, but with a domain-specific talent for coding and AI research) to radical superintelligence.</p>
</li>
<li><p><strong>Whole brain emulation (&quot;uploading&quot;):</strong> Intelligent software would be produced by scanning and closely modeling the computational structure of a biological brain. The whole brain emulation path does not require that we figure out how human cognition works or how to program an artificial intelligence. It requires only that we understand the low-level functional characteristics of the basic computational elements of the brain. No fundamental conceptual or theoretical breakthrough is needed for whole brain emulation to succeed. In general, whole brain emulation relies less on theoretical insight and more on technological capability than artificial intelligence.</p>
</li>
<li><p><strong>Biological cognition:</strong> A third path to greater-than-current-human intelligence is to enhance the functioning of biological brains. The generational lag in germline interventions means that progress could not be nearly as sudden and abrupt as in scenarios involving machine intelligence.</p>
</li>
<li><p><strong>Brainâ€“computer interfaces</strong></p>
</li>
<li><p><strong>Networks and organizations:</strong> The gradual enhancement of networks and organizations that link individual human minds with one another and with various artifacts and bots. The idea here is not that this would enhance the intellectual capacity of individuals enough to make them superintelligent, but rather that some system composed of individuals thus networked and organized might attain a form of superintelligence--what in the next chapter we will elaborate as &quot;collective superintelligence.&quot;</p>
</li>
</ul>
<p><strong>The superintelligent will</strong></p>
<p><em>The instrumental convergence thesis:</em> Several instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent&#39;s goal being realized for a wide range of final goals and a wide range of situations, implying that these instrumental values are likely to be pursued by a broad spectrum of situated intelligent agents.</p>
<ul>
<li><p>Self-preservation</p>
</li>
<li><p>Goal-content integrity</p>
</li>
<li><p>Cognitive enhancement</p>
</li>
<li><p>Technological perfection</p>
</li>
<li><p>Resource acquisition</p>
</li>
</ul>
<p><strong>Is the default outcome doom?</strong></p>
<p>The first superintelligence may shape the future of Earth-originating life, could easily have non-anthropomorphic final goals, and would likely have instrumental reasons to pursue open-ended resource acquisition. If we now reflect that human beings consist of useful resources (such as conveniently located atoms) and that we depend for our survival and flourishing on many more local resources, we can see that the outcome could easily be one in which humanity quickly becomes extinct.</p>
<p><em>The treacherous turn:</em> While weak, an AI behaves cooperatively (increasingly so, as it gets smarter). When the AI gets sufficiently strong--without warning or provocation--it strikes, forms a singleton, and begins directly to optimize the world according to the criteria implied by its final values.</p>
<p><strong>The control problem</strong></p>
<p>We can divide potential control methods into two broad classes: capability control methods, which aim to control what the superintelligence can do; and motivation selection methods, which aim to control what it wants to do.</p>
<p>It is important to realize that some control method (or combination of methods) must be implemented before the system becomes superintelligent. It cannot be done after the system has obtained a decisive strategic advantage.</p>
<p>Motivation selection can involve explicitly formulating a goal or set of rules to be followed (direct specification) or setting up the system so that it can discover an appropriate set of values for itself by reference to some implicitly or indirectly formulated criterion (indirect normativity). One option in motivation selection is to try to build the system so that it would have modest, non-ambitious goals (domesticity). An alternative to creating a motivation system from scratch is to select an agent that already has an acceptable motivation system and then augment that agent&#39;s cognitive powers to make it superintelligent, while ensuring that the motivation system does not get corrupted in the process (augmentation).</p>
<p><strong>Acquiring values</strong></p>
<p><em>The principle of epistemic deference:</em> A future superintelligence occupies an epistemically superior vantage point: its beliefs are (probably, on most topics) more likely than ours to be true. We should therefore defer to the superintelligence&#39;s opinion whenever feasible.</p>
<p>Indirect normativity applies this principle to the value-selection problem. Lacking confidence in our ability to specify a concrete normative standard, we would instead specify some more abstract condition that any normative standard should satisfy, in the hope that a superintelligence could find a concrete standard that satisfies the abstract condition. We could give a seed AI the final goal of continuously acting according to its best estimate of what this implicitly defined standard would have it do.</p>
<p><strong>Will the best in human nature please stand up</strong></p>
<p>Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb. Such is the mismatch between the power of our plaything and the immaturity of our conduct. Superintelligence is a challenge for which we are not ready now and will not be ready for a long time. We have little idea when the detonation will occur, though if we hold the device to our ear we can hear a faint ticking sound.</p>

            </section>
        </main>
    </div>
</body>

</html>